{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "512e06e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mke37/.local/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "2024-06-12 22:49:58.770560: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-12 22:49:58.770662: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-12 22:49:58.772502: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-12 22:49:58.785842: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-12 22:50:01.868019: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/mke37/.local/lib/python3.10/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from preprocess import NERDataset\n",
    "from cleaning import DataReader\n",
    "import numpy as np\n",
    "from utils import compute_metrics, get_label_map, get_inv_label_map, read_labels\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, BertForTokenClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "from config import Config\n",
    "\n",
    "from transformers import AutoTokenizer, BertForTokenClassification # Import AutoTokenizer and BertForTokenClassification from the transformers library for NLP tasks.\n",
    "import torch # Import the PyTorch library for tensor computations and deep learning.\n",
    "import numpy as np # Import NumPy for numerical operations and array manipulations.\n",
    "import argparse # Import argparse for parsing command-line arguments.\n",
    "from typing import List # Import List from the typing module for type annotations.\n",
    "from config import Config # Import Config class from the config module, used for loading and accessing configuration settings.\n",
    "# Import utility functions: read_labels (to read label data), get_label_map and get_inv_label_map (for mapping labels to indices and vice versa).\n",
    "from utils import read_labels, get_label_map, get_inv_label_map\n",
    "import argparse # Re-import argparse (duplicate import, not necessary).\n",
    "import sys # Import sys for interacting with the Python interpreter (e.g., command-line arguments, system exit).\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "import re\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "981fe364-a328-428b-a3a4-4e20b247a353",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "labels = []\n",
    "\n",
    "curr_sentence = []\n",
    "curr_labels = []\n",
    "\n",
    "with open(\"LabeledArticles.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        if line != \"\\n\":\n",
    "            label = line.split()[0]\n",
    "            word = line.split()[1]\n",
    "            \n",
    "            curr_sentence.append(word)\n",
    "            curr_labels.append(label)\n",
    "        else:\n",
    "            sentences.append(curr_sentence)\n",
    "            labels.append(curr_labels)\n",
    "            curr_sentence = []\n",
    "            curr_labels = []\n",
    "            \n",
    "print(\"DONE!\")           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4d6ee6f-85ed-48bf-8dda-d342f0f854e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5545 5545\n",
      "['ولهذا', 'السبب', 'يمكن', 'وصف', 'مثبطات', 'إعادة', 'امتصاص', 'السيروتونين', 'الانتقائية', 'لعلاج', 'سرعة', 'القذف', '.']\n",
      "['OUTSIDE', 'OUTSIDE', 'OUTSIDE', 'OUTSIDE', 'OUTSIDE', 'OUTSIDE', 'OUTSIDE', 'OUTSIDE', 'OUTSIDE', 'OUTSIDE', 'OUTSIDE', 'OUTSIDE', 'OUTSIDE']\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences), len(labels))\n",
    "print(sentences[-1])\n",
    "print(labels[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b29445eb-47f8-4547-a7ee-959a7ff9460a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_diacritics(text):\n",
    "    return re.sub(re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]'),\"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "812d67ae-7f4f-43e5-a115-0e25656e13f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences[i])):\n",
    "        sentences[i][j] = remove_diacritics(sentences[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6eaee22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: 5545 , Sentences: 5545 , Tags: 5545\n",
      "Data: 856 , Sentences: 856 , Tags: 856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/sw/miniconda/envs/transformers-r1/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f35285ef15a4bf59cc6bf840ee786cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.49186927400078484\n",
      "Evaluating Epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4346e38cc0f4d4bbdf0bf1bf924f1da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mke37/.local/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: OUTSIDE seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 0.677577295375091\n",
      "Eval Metrics: {'accuracy_score': 0.8651133067885759, 'precision': 0.6013986013986014, 'recall': 0.2986111111111111, 'f1': 0.39907192575406036}\n",
      "Training Epoch: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b539cae3207a4127b23e5f08adfbfbff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2530198693490166\n",
      "Evaluating Epoch: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ac8ef4d554b40aa8084c4529d1f136d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 0.7473255317244265\n",
      "Eval Metrics: {'accuracy_score': 0.8583296465614708, 'precision': 0.6221498371335505, 'recall': 0.24869791666666666, 'f1': 0.3553488372093023}\n",
      "Training Epoch: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70d7dd493c8c4b7b90aff5bcc3e83e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.18576182571027738\n",
      "Evaluating Epoch: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78de6942d1b44f14bd6c5c291107f0ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 0.7138166056463012\n",
      "Eval Metrics: {'accuracy_score': 0.8740107162168805, 'precision': 0.5709436524100475, 'recall': 0.3650173611111111, 'f1': 0.445326979083929}\n",
      "Training Epoch: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d9f536f3f26475f8d3e211c8be81afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.14486933959062917\n",
      "Evaluating Epoch: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f7f774343940afacd15d2deab51b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 0.7571508742317006\n",
      "Eval Metrics: {'accuracy_score': 0.873715774467876, 'precision': 0.5704371963913949, 'recall': 0.3567708333333333, 'f1': 0.4389853137516688}\n",
      "Training Epoch: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df4803c93b24455c82356b7c6b32e2c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.11597059962342383\n",
      "Evaluating Epoch: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae7354aa745146afad9a49bc96b3e12d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 0.8275695371407049\n",
      "Eval Metrics: {'accuracy_score': 0.8710612987268348, 'precision': 0.5771583956492182, 'recall': 0.3684895833333333, 'f1': 0.4498013245033112}\n",
      "Training Epoch: 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad05111e821f41fb9eff7c964fdb4a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.09835844851965864\n",
      "Evaluating Epoch: 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "999a8506f15844ec88cf89c0aed9a75d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 0.844508489800824\n",
      "Eval Metrics: {'accuracy_score': 0.8691441773583051, 'precision': 0.578113750899928, 'recall': 0.3485243055555556, 'f1': 0.43487679393447065}\n",
      "Training Epoch: 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5020b7e48bda46d5b7cd88204cdf7acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.08535226479559192\n",
      "Evaluating Epoch: 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0149d6feb5654a30b73790576f2a0615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 0.9428383808720995\n",
      "Eval Metrics: {'accuracy_score': 0.8743056579658851, 'precision': 0.5926193921852387, 'recall': 0.35546875, 'f1': 0.44438415626695604}\n",
      "Training Epoch: 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a390daaa5d5143c2a3ed4b807efb6cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.07394821992042\n",
      "Evaluating Epoch: 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc0df1e0124c4ca28c95896884a0b208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 1.0141782059713647\n",
      "Eval Metrics: {'accuracy_score': 0.8720444378901834, 'precision': 0.5779411764705882, 'recall': 0.3411458333333333, 'f1': 0.4290393013100437}\n",
      "Training Epoch: 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f8d195fd8804ef2ada62e896c98e412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.06633550884948254\n",
      "Evaluating Epoch: 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a720bde41bcf42e58142440075429a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 1.0303013799367127\n",
      "Eval Metrics: {'accuracy_score': 0.8706188861033279, 'precision': 0.5721500721500722, 'recall': 0.3441840277777778, 'f1': 0.42981029810298105}\n",
      "Training Epoch: 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38a4a05b14844391b8cc8190bfe43cf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.06048656450396656\n",
      "Evaluating Epoch: 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c075c32493a94b089afb866fc085c578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 1.0652938159527603\n",
      "Eval Metrics: {'accuracy_score': 0.8696357469399794, 'precision': 0.5922712933753943, 'recall': 0.3259548611111111, 'f1': 0.42049272116461367}\n",
      "Training Epoch: 11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e27fe44bee2479a8e3b3363c511e5e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.05460819926230983\n",
      "Evaluating Epoch: 11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30896e125749438e9232e72c49a9ad5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 1.0725616520201717\n",
      "Eval Metrics: {'accuracy_score': 0.8704222582706582, 'precision': 0.5864381520119225, 'recall': 0.3415798611111111, 'f1': 0.4317059791552386}\n",
      "Training Epoch: 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5858aa06fca421097f8b135653c3998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.051028356632427115\n",
      "Evaluating Epoch: 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "763d7a00777a41d28857e764cd6f31a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 1.105601602682361\n",
      "Eval Metrics: {'accuracy_score': 0.8688983925674679, 'precision': 0.580015026296018, 'recall': 0.3350694444444444, 'f1': 0.4247592847317744}\n",
      "Training Epoch: 13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3256e70be9a04df097d4d2df4def2fd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.046968668892507254\n",
      "Evaluating Epoch: 13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8db6b05046ff42a6b2c95f0d695621b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 1.0919076518879995\n",
      "Eval Metrics: {'accuracy_score': 0.8720935948483508, 'precision': 0.5833333333333334, 'recall': 0.3524305555555556, 'f1': 0.4393939393939394}\n",
      "Training Epoch: 14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b9c4aacf90c4d17bab59864fc32efbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.04389789639180392\n",
      "Evaluating Epoch: 14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f88282ec7ebd43fab8eb092e2ec9b17a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 1.1304601472285059\n",
      "Eval Metrics: {'accuracy_score': 0.8710612987268348, 'precision': 0.5792857142857143, 'recall': 0.3519965277777778, 'f1': 0.4379049676025918}\n",
      "Training Epoch: 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d30f30dcac594bedae4c3a1167265188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.042181180105665406\n",
      "Evaluating Epoch: 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae76991d7aa049959dab1bd5146c92d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 1.154604548657382\n",
      "Eval Metrics: {'accuracy_score': 0.8731258909698668, 'precision': 0.5792466240227434, 'recall': 0.3537326388888889, 'f1': 0.4392347076259769}\n",
      "Training Epoch: 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dbe4705df5a4cb0a11fe389575dedb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.03971516852527427\n",
      "Evaluating Epoch: 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff89a5667e044c8d8d095db592706365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 1.2351127827057131\n",
      "Eval Metrics: {'accuracy_score': 0.8704222582706582, 'precision': 0.5934897804693414, 'recall': 0.3402777777777778, 'f1': 0.432551724137931}\n",
      "Training Epoch: 17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9479b4315b74e208385a83a36e75000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.03703033853045823\n",
      "Evaluating Epoch: 17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ad320cb98d94371a4c31f7a42437dbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 1.226512058465569\n",
      "Eval Metrics: {'accuracy_score': 0.8728309492208622, 'precision': 0.5912921348314607, 'recall': 0.3654513888888889, 'f1': 0.45171673819742486}\n",
      "Training Epoch: 18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34b1940a584c42dfb1f4c1a6b928dbcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.03527867691108275\n",
      "Evaluating Epoch: 18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "847b8293d09d40c4847f52fdb285327f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 1.2720195355790633\n",
      "Eval Metrics: {'accuracy_score': 0.8715037113503417, 'precision': 0.5932203389830508, 'recall': 0.3493923611111111, 'f1': 0.43977055449330776}\n",
      "Training Epoch: 19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baab33c153444a36a3af592b8f5e4b3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.03365460202676279\n",
      "Evaluating Epoch: 19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e3a450cd0d54a02b0786fe73b4497dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 1.2596259738008182\n",
      "Eval Metrics: {'accuracy_score': 0.8717986530993462, 'precision': 0.5888252148997135, 'recall': 0.3567708333333333, 'f1': 0.44432432432432434}\n",
      "Training Epoch: 20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0a4aa9565a64975a0e46d24a4f7a694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.03227192311687261\n",
      "Evaluating Epoch: 20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a9fe75cc1064b5e8057159fd6722353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 1.2755796890015956\n",
      "Eval Metrics: {'accuracy_score': 0.8717003391830114, 'precision': 0.5893371757925072, 'recall': 0.3550347222222222, 'f1': 0.44312026002166843}\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "class NERTrainer:\n",
    "    def __init__(self, test_dataset_path):\n",
    "        self.cfg = Config()\n",
    "        self.data_reader = DataReader(\"LabeledArticles.txt\")\n",
    "        self.data, _, _ = self.data_reader.read_data_bert()\n",
    "        self.label_list = read_labels('NewEntities.txt')\n",
    "\n",
    "        self.label_map = get_label_map(self.label_list)\n",
    "        self.inv_label_map = get_inv_label_map(self.label_list)\n",
    "\n",
    "        # Load the test dataset\n",
    "        self.test_data_reader = DataReader(test_dataset_path)\n",
    "        self.test_data, _, _ = self.test_data_reader.read_data_bert()\n",
    "\n",
    "        self.TOKENIZER = AutoTokenizer.from_pretrained(self.cfg.MODEL_NAME)\n",
    "\n",
    "        self.train_dataset = NERDataset(\n",
    "            texts=[x[0] for x in self.data],\n",
    "            tags=[x[1] for x in self.data],\n",
    "            label_list=self.label_list,\n",
    "            model_name=self.cfg.MODEL_NAME,\n",
    "            max_length=self.cfg.MAX_LEN\n",
    "        )\n",
    "\n",
    "        self.test_dataset = NERDataset(\n",
    "            texts=[x[0] for x in self.test_data],\n",
    "            tags=[x[1] for x in self.test_data],\n",
    "            label_list=self.label_list,\n",
    "            model_name=self.cfg.MODEL_NAME,\n",
    "            max_length=self.cfg.MAX_LEN\n",
    "        )\n",
    "\n",
    "        self.train_data_loader = DataLoader(dataset=self.train_dataset, batch_size=self.cfg.TRAIN_BATCH_SIZE, shuffle=True)\n",
    "        self.test_data_loader = DataLoader(dataset=self.test_dataset, batch_size=self.cfg.VALID_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        self.model = BertForTokenClassification.from_pretrained(self.cfg.MODEL_NAME,\n",
    "                                                                return_dict=True,\n",
    "                                                                num_labels=len(self.label_map),\n",
    "                                                                output_attentions=False,\n",
    "                                                                output_hidden_states=False).to(self.cfg.device)\n",
    "\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=5e-5, correct_bias=False)\n",
    "        total_steps = len(self.train_data_loader) * self.cfg.EPOCHS\n",
    "\n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "\n",
    "        self.best_eval_loss = float('inf')\n",
    "        self.best_model = None\n",
    "\n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        final_loss = 0\n",
    "\n",
    "        for data in tqdm(self.train_data_loader, total=len(self.train_data_loader)):\n",
    "            input_ids = data['input_ids'].to(self.cfg.device)\n",
    "            attention_mask = data['attention_mask'].to(self.cfg.device)\n",
    "            token_type_ids = data['token_type_ids'].to(self.cfg.device)\n",
    "            labels = data['labels'].to(self.cfg.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(input_ids=input_ids,\n",
    "                                 token_type_ids=token_type_ids,\n",
    "                                 attention_mask=attention_mask,\n",
    "                                 labels=labels)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            final_loss += loss.item()\n",
    "\n",
    "        loss = final_loss / len(self.train_data_loader)\n",
    "        print(f\"Train loss: {loss}\")\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def eval_epoch(self):\n",
    "        self.model.eval()\n",
    "        final_loss = 0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in tqdm(self.test_data_loader, total=len(self.test_data_loader)):\n",
    "                input_ids = data['input_ids'].to(self.cfg.device)\n",
    "                attention_mask = data['attention_mask'].to(self.cfg.device)\n",
    "                token_type_ids = data['token_type_ids'].to(self.cfg.device)\n",
    "                labels = data['labels'].to(self.cfg.device)\n",
    "\n",
    "                outputs = self.model(input_ids=input_ids,\n",
    "                                     token_type_ids=token_type_ids,\n",
    "                                     attention_mask=attention_mask,\n",
    "                                     labels=labels)\n",
    "\n",
    "                loss = outputs.loss\n",
    "                final_loss += loss.item()\n",
    "\n",
    "                logits = outputs.logits.detach().cpu().numpy()\n",
    "                labels = labels.to('cpu').numpy()\n",
    "\n",
    "                all_preds.extend(logits)\n",
    "                all_labels.extend(labels)\n",
    "\n",
    "        all_preds = np.array(all_preds)\n",
    "        all_labels = np.asarray(all_labels)\n",
    "\n",
    "        metrics = compute_metrics(all_preds, all_labels, self.inv_label_map, False)\n",
    "        final_loss = final_loss / len(self.test_data_loader)\n",
    "\n",
    "        print(f\"Eval loss: {final_loss}\")\n",
    "        print(f\"Eval Metrics: {metrics}\")\n",
    "\n",
    "        return final_loss, metrics\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.cfg.EPOCHS):\n",
    "            print(f\"Training Epoch: {epoch + 1}\")\n",
    "            self.train_epoch()\n",
    "\n",
    "            print(f\"Evaluating Epoch: {epoch + 1}\")\n",
    "            eval_loss, _ = self.eval_epoch()\n",
    "\n",
    "            if eval_loss < self.best_eval_loss:\n",
    "                self.best_eval_loss = eval_loss\n",
    "                self.best_model = self.model.state_dict()\n",
    "                torch.save(self.best_model, \"TeacherModel.pt\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_dataset_path = \"TestingData.txt\"  # Path to the custom test dataset\n",
    "    ner_trainer = NERTrainer(test_dataset_path)\n",
    "    ner_trainer.train()\n",
    "    print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e446e9f1-e1db-4115-85d5-3d5963b5682e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "labels = []\n",
    "\n",
    "curr_sentence = []\n",
    "curr_labels = []\n",
    "\n",
    "with open(\"SemiLabeledArticles.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        if line != \"\\n\":\n",
    "            label = line.split()[0]\n",
    "            word = line.split()[1]\n",
    "            \n",
    "            curr_sentence.append(word)\n",
    "            curr_labels.append(label)\n",
    "        else:\n",
    "            sentences.append(curr_sentence)\n",
    "            labels.append(curr_labels)\n",
    "            curr_sentence = []\n",
    "            curr_labels = []\n",
    "            \n",
    "print(\"DONE!\")           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e007cbce-be99-4b85-b0dd-2457b874b6a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21724 21724\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences), len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e73bfd15-f1fb-42ee-a417-34d9998e9349",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_diacritics(text):\n",
    "    return re.sub(re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]'),\"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d81b7084-a4c8-4bc8-8a6b-614491275aeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 %\n",
      "0.46 %\n",
      "0.92 %\n",
      "1.38 %\n",
      "1.84 %\n",
      "2.3 %\n",
      "2.76 %\n",
      "3.22 %\n",
      "3.68 %\n",
      "4.14 %\n",
      "4.6 %\n",
      "5.06 %\n",
      "5.52 %\n",
      "5.98 %\n",
      "6.44 %\n",
      "6.9 %\n",
      "7.37 %\n",
      "7.83 %\n",
      "8.29 %\n",
      "8.75 %\n",
      "9.21 %\n",
      "9.67 %\n",
      "10.13 %\n",
      "10.59 %\n",
      "11.05 %\n",
      "11.51 %\n",
      "11.97 %\n",
      "12.43 %\n",
      "12.89 %\n",
      "13.35 %\n",
      "13.81 %\n",
      "14.27 %\n",
      "14.73 %\n",
      "15.19 %\n",
      "15.65 %\n",
      "16.11 %\n",
      "16.57 %\n",
      "17.03 %\n",
      "17.49 %\n",
      "17.95 %\n",
      "18.41 %\n",
      "18.87 %\n",
      "19.33 %\n",
      "19.79 %\n",
      "20.25 %\n",
      "20.71 %\n",
      "21.17 %\n",
      "21.64 %\n",
      "22.1 %\n",
      "22.56 %\n",
      "23.02 %\n",
      "23.48 %\n",
      "23.94 %\n",
      "24.4 %\n",
      "24.86 %\n",
      "25.32 %\n",
      "25.78 %\n",
      "26.24 %\n",
      "26.7 %\n",
      "27.16 %\n",
      "27.62 %\n",
      "28.08 %\n",
      "28.54 %\n",
      "29.0 %\n",
      "29.46 %\n",
      "29.92 %\n",
      "30.38 %\n",
      "30.84 %\n",
      "31.3 %\n",
      "31.76 %\n",
      "32.22 %\n",
      "32.68 %\n",
      "33.14 %\n",
      "33.6 %\n",
      "34.06 %\n",
      "34.52 %\n",
      "34.98 %\n",
      "35.44 %\n",
      "35.9 %\n",
      "36.37 %\n",
      "36.83 %\n",
      "37.29 %\n",
      "37.75 %\n",
      "38.21 %\n",
      "38.67 %\n",
      "39.13 %\n",
      "39.59 %\n",
      "40.05 %\n",
      "40.51 %\n",
      "40.97 %\n",
      "41.43 %\n",
      "41.89 %\n",
      "42.35 %\n",
      "42.81 %\n",
      "43.27 %\n",
      "43.73 %\n",
      "44.19 %\n",
      "44.65 %\n",
      "45.11 %\n",
      "45.57 %\n",
      "46.03 %\n",
      "46.49 %\n",
      "46.95 %\n",
      "47.41 %\n",
      "47.87 %\n",
      "48.33 %\n",
      "48.79 %\n",
      "49.25 %\n",
      "49.71 %\n",
      "50.17 %\n",
      "50.64 %\n",
      "51.1 %\n",
      "51.56 %\n",
      "52.94 %\n",
      "53.4 %\n",
      "53.86 %\n",
      "54.32 %\n",
      "54.78 %\n",
      "55.24 %\n",
      "55.7 %\n",
      "56.16 %\n",
      "56.62 %\n",
      "57.08 %\n",
      "57.54 %\n",
      "58.0 %\n",
      "58.46 %\n",
      "58.92 %\n",
      "59.38 %\n",
      "59.84 %\n",
      "60.3 %\n",
      "60.76 %\n",
      "61.22 %\n",
      "61.68 %\n",
      "62.14 %\n",
      "62.6 %\n",
      "63.06 %\n",
      "63.52 %\n",
      "63.98 %\n",
      "64.44 %\n",
      "64.91 %\n",
      "65.37 %\n",
      "65.83 %\n",
      "66.29 %\n",
      "66.75 %\n",
      "67.21 %\n",
      "67.67 %\n",
      "68.13 %\n",
      "68.59 %\n",
      "69.05 %\n",
      "69.51 %\n",
      "69.97 %\n",
      "70.43 %\n",
      "70.89 %\n",
      "71.35 %\n",
      "71.81 %\n",
      "72.27 %\n",
      "72.73 %\n",
      "73.19 %\n",
      "73.65 %\n",
      "74.11 %\n",
      "74.57 %\n",
      "75.03 %\n",
      "75.49 %\n",
      "75.95 %\n",
      "76.41 %\n",
      "76.87 %\n",
      "77.33 %\n",
      "77.79 %\n",
      "78.25 %\n",
      "78.71 %\n",
      "79.18 %\n",
      "79.64 %\n",
      "80.1 %\n",
      "80.56 %\n",
      "81.02 %\n",
      "81.48 %\n",
      "81.94 %\n",
      "82.4 %\n",
      "82.86 %\n",
      "83.32 %\n",
      "83.78 %\n",
      "84.24 %\n",
      "84.7 %\n",
      "85.16 %\n",
      "85.62 %\n",
      "86.08 %\n",
      "86.54 %\n",
      "87.0 %\n",
      "87.46 %\n",
      "87.92 %\n",
      "88.38 %\n",
      "88.84 %\n",
      "89.3 %\n",
      "89.76 %\n",
      "90.22 %\n",
      "90.68 %\n",
      "91.14 %\n",
      "91.6 %\n",
      "92.06 %\n",
      "92.52 %\n",
      "92.98 %\n",
      "93.45 %\n",
      "93.91 %\n",
      "94.37 %\n",
      "94.83 %\n",
      "95.29 %\n",
      "95.75 %\n",
      "96.21 %\n",
      "96.67 %\n",
      "97.13 %\n",
      "97.59 %\n",
      "98.05 %\n",
      "98.51 %\n",
      "98.97 %\n",
      "99.43 %\n",
      "99.89 %\n",
      "100 %\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "class NERPredictor:\n",
    "    def __init__(self, model_path: str):\n",
    "        self.cfg = Config() # Initialize and load configuration settings from the Config class.\n",
    "        \n",
    "        # Read the label list from the specified file path.\n",
    "        self.label_list = read_labels('NewEntities.txt')\n",
    "        # Create mappings from labels to indices and vice versa.\n",
    "        self.label_map = get_label_map(self.label_list)\n",
    "        self.inv_label_map = get_inv_label_map(self.label_list)\n",
    "\n",
    "        # Load the pre-trained BERT model for token classification.\n",
    "        self.model = BertForTokenClassification.from_pretrained(\n",
    "            self.cfg.MODEL_NAME,\n",
    "            return_dict=True,\n",
    "            num_labels=len(self.label_map),\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False\n",
    "        )\n",
    "\n",
    "        # Load the saved model weights.\n",
    "        self.model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "        # Load the tokenizer associated with the pre-trained BERT model.\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.cfg.MODEL_NAME)\n",
    "\n",
    "    def predict(self, sentences: str) -> Tuple[List[str], List[float]]:\n",
    "\n",
    "        \n",
    "        foundDecimal = False\n",
    "        \n",
    "        tokenArray = sentences.split()\n",
    "        \n",
    "        for i in range(0, len(tokenArray)):\n",
    "            tokenArray[i] = remove_diacritics(tokenArray[i])\n",
    "            \n",
    "        sentences = ' '.join(tokenArray)\n",
    "        # print(sentences)\n",
    "        \n",
    "        # Tokenize the input sentence to get input IDs.\n",
    "        input_ids = self.tokenizer.encode(sentences, return_tensors='pt')\n",
    "        # print(len(input_ids[0]), input_ids)\n",
    "        with torch.no_grad(): # Disable gradient calculations for inference.\n",
    "            self.model.to('cpu') # Ensure the model is on CPU for inference.\n",
    "            # Get model predictions for the input IDs.\n",
    "            output = self.model(input_ids)\n",
    "\n",
    "        logits = output.logits.to('cpu')  # Get logits from the model's output\n",
    "        probabilities = F.softmax(logits, dim=-1)  # Apply softmax to get probabilities\n",
    "       \n",
    "        # Extract predicted labels and confidence scores\n",
    "        label_indices = torch.argmax(logits, dim=-1).numpy()\n",
    "        max_probabilities, predicted_labels = torch.max(probabilities, dim=-1)\n",
    "        confidence_scores = max_probabilities.numpy()\n",
    "\n",
    "    \n",
    "        # Convert model logits to label indices.\n",
    "        # label_indices = np.argmax(output.logits.to('cpu').numpy(), axis=2)\n",
    "        # Convert input IDs back to tokens.\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
    "\n",
    "        new_tokens, new_labels, new_confidence_scores = [], [], []\n",
    "        merges = []\n",
    "        \n",
    "        new_confidence_scores = confidence_scores[0].copy()\n",
    "        \n",
    "        label_indices_0 = label_indices[0]\n",
    "        for i in range(len(tokens)):\n",
    "            token = tokens[i]\n",
    "            label_idx = label_indices_0[i]\n",
    "            # Merge subword tokens that start with \"##\".\n",
    "            # print(token, label_idx, \"DEBUG\", i)\n",
    "            # print(token, \"DEBUG\", i)\n",
    "\n",
    "            if foundDecimal:\n",
    "                # print(\"__________foundDecimal__________\")\n",
    "                token = \"##\" + tokens[i]\n",
    "                foundDecimal = False\n",
    "            if token.startswith(\".\") and tokens[i-1][-1].isdigit() and i != len(tokens)-2:\n",
    "                # print(\"__________foundDecimal__________\")\n",
    "                foundDecimal = True\n",
    "                token = \"##\" + tokens[i]\n",
    "            # Disabled below as that means there's an error in the sentences itself\n",
    "            if token == \"%\" or token == \"٪\" and i != 1:\n",
    "                # print(\"________foundPercentage_________\")\n",
    "                token = \"##\" + tokens[i]\n",
    "                # print(token)\n",
    "            if (token == \"٬\" or token == \"٫\") and (any(char.isdigit() for char in prev_token) and any(char.isdigit() for char in tokens[i+1])):\n",
    "                # print(\"___________foundComma___________\")\n",
    "                token = \"##\" + tokens[i]\n",
    "                foundDecimal = True\n",
    "                \n",
    "            elif (token == \"٬\" or token == \"٫\") and not (any(char.isdigit() for char in prev_token) and any(char.isdigit() for char in tokens[i+1])):\n",
    "                # print(\"___________foundComma___________\")\n",
    "                continue\n",
    "                \n",
    "            if token == \"؟\" or token == \"?\":\n",
    "                # print(\"___________foundQuestionMark___________\")\n",
    "                token = \"##\" + tokens[i]                            \n",
    "            if token.startswith(\"##\"):\n",
    "                merges.append(i)\n",
    "                # print(\"_____________MERGE_______________\")\n",
    "                new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "\n",
    "            else:\n",
    "                if input_ids[0][i] == 2 or input_ids[0][i] == 3:\n",
    "                    continue\n",
    "                # Append the label for the token to new_labels.\n",
    "                new_labels.append(self.inv_label_map[label_idx])\n",
    "                # Append the token to new_tokens.\n",
    "                new_tokens.append(token)\n",
    "\n",
    "\n",
    "                \n",
    "            prev_token = token   \n",
    "            \n",
    "        new_confidence_scores = confidence_scores[0].copy()\n",
    "\n",
    "        # Iterate through confidence scores\n",
    "        i = 0\n",
    "        while i < len(new_confidence_scores):\n",
    "            # Check if current index is a merge point\n",
    "            if i in merges:\n",
    "                # Determine which element to keep and delete the other\n",
    "                if new_confidence_scores[i] >= new_confidence_scores[i - 1]:\n",
    "                    new_confidence_scores = np.delete(new_confidence_scores, i - 1)\n",
    "                    i -= 1  # Adjust index after deletion\n",
    "                else:\n",
    "                    new_confidence_scores = np.delete(new_confidence_scores, i)\n",
    "                    i -= 1  # Adjust index after deletion\n",
    "                # Adjust merge indices\n",
    "                merges = [merge - 1 for merge in merges if merge > i]\n",
    "            i += 1  # Move to the next element\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "        # Return the list of labels corresponding to each token in the input.\n",
    "        return new_tokens, new_labels, new_confidence_scores[1:-1]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    predictor = NERPredictor(model_path='TeacherModel.pt')\n",
    "\n",
    "    # index = 10\n",
    "    # curr_tokens, curr_predicted_labels, curr_confidence_scores = predictor.predict(' '.join(sentences[index]))\n",
    "    # # predictions[index] = curr_predicted_labels\n",
    "    # print(len(curr_predicted_labels), len(curr_confidence_scores), len(sentences[index]))\n",
    "    # print(curr_tokens, curr_predicted_labels, curr_confidence_scores)       \n",
    "    \n",
    "    \n",
    "    predictions = []\n",
    "    confidence_scores = []\n",
    "    copySentences = []\n",
    "    for i in range(0, len(sentences)):\n",
    "        curr_tokens, curr_predicted_labels, curr_confidence_scores = predictor.predict(' '.join(sentences[i]))\n",
    "        predictions.append(curr_predicted_labels)\n",
    "        copySentences.append(curr_tokens)\n",
    "        confidence_scores.append(curr_confidence_scores)\n",
    "        if i % 100 == 0:\n",
    "            print(round((i/len(sentences)) * 100, 2), \"%\")\n",
    "        \n",
    "    print(\"100 %\")\n",
    "    print(\"Done!\")\n",
    "\n",
    "    # index = 4\n",
    "    # index = 4054\n",
    "    # index = 5105\n",
    "    # index = 6948\n",
    "    # index = 9796\n",
    "    # index = 12370\n",
    "    # index = 14121\n",
    "    # index = 18021\n",
    "    # index = 20745\n",
    "    # index = 253\n",
    "    # index = 261\n",
    "    # index = 268\n",
    "    \n",
    "#     curr_tokens, curr_predicted_labels, curr_confidence_scores = predictor.predict(' '.join(sentences[index]))\n",
    "\n",
    "#     # Print titles\n",
    "#     print(f\"{'Original Token':<20} {'True Label':<20} {'Word':<20} {'Prediction':<20}\")\n",
    "\n",
    "#     # Print tokens and corresponding labels\n",
    "#     for i in range(0, len(sentences[index])):\n",
    "#         print(f\"{sentences[index][i]:<20} {labels[index][i]:<20} {curr_tokens[i]:<20} {curr_predicted_labels[i]:<20}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db3c0443-b58c-41d2-a501-a95ba7118184",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "# dill.load_session('June9STeachingCompleted.db')\n",
    "dill.dump_session('TeachingCompleted.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73bee9ab-8e49-45ac-98c8-ee75c87c6fec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Token       True Label           Word                 Prediction          \n",
      "إنجلترا              B-Country            إنجلترا              B-Country           \n",
      "أو                   OUTSIDE              أو                   OUTSIDE             \n",
      "إنكلترا              B-Country            إنكلترا              B-Country           \n",
      "أو                   OUTSIDE              أو                   OUTSIDE             \n",
      "إنكلترة              B-Country            إنكلترة              B-Country           \n",
      "بالإنجليزية          B-Language           بالإنجليزية          B-Language          \n",
      "إنگلاند              OUTSIDE              [UNK]                OUTSIDE             \n",
      "هي                   OUTSIDE              هي                   OUTSIDE             \n",
      "أكبر                 OUTSIDE              أكبر                 OUTSIDE             \n",
      "دولة                 OUTSIDE              دولة                 OUTSIDE             \n",
      "في                   OUTSIDE              في                   OUTSIDE             \n",
      "المملكة              B-Country            المملكة              B-Country           \n",
      "المتحدة              I-Country            المتحدة              I-Country           \n",
      "وتشترك               OUTSIDE              وتشترك               OUTSIDE             \n",
      "في                   OUTSIDE              في                   OUTSIDE             \n",
      "الحدود               OUTSIDE              الحدود               OUTSIDE             \n",
      "البرية               OUTSIDE              البرية               OUTSIDE             \n",
      "مع                   OUTSIDE              مع                   OUTSIDE             \n",
      "اسكتلندا             B-Country            اسكتلندا             B-Country           \n",
      "في                   OUTSIDE              في                   OUTSIDE             \n",
      "الشمال               OUTSIDE              الشمال               OUTSIDE             \n",
      "وويلز                OUTSIDE              وويلز                OUTSIDE             \n",
      "في                   OUTSIDE              في                   OUTSIDE             \n",
      "الغرب                OUTSIDE              الغرب                OUTSIDE             \n",
      "والبحر               B-Body_Of_Water      والبحر               B-Body_Of_Water     \n",
      "الأيرلندي            I-Body_Of_Water      الأيرلندي            I-Body_Of_Water     \n",
      "في                   OUTSIDE              في                   OUTSIDE             \n",
      "الشمال               OUTSIDE              الشمال               OUTSIDE             \n",
      "الغربي               OUTSIDE              الغربي               OUTSIDE             \n",
      "وبحر                 B-Body_Of_Water      وبحر                 B-Body_Of_Water     \n",
      "الكلت                I-Body_Of_Water      الكلت                I-Body_Of_Water     \n",
      "في                   OUTSIDE              في                   OUTSIDE             \n",
      "الجنوب               OUTSIDE              الجنوب               OUTSIDE             \n",
      "الغربي               OUTSIDE              الغربي               OUTSIDE             \n",
      "وبحر                 OUTSIDE              وبحر                 B-Body_Of_Water     \n",
      "الشمال               OUTSIDE              الشمال               I-Body_Of_Water     \n",
      "في                   OUTSIDE              في                   OUTSIDE             \n",
      "الشرق                OUTSIDE              الشرق                OUTSIDE             \n",
      "وتفصلها              OUTSIDE              وتفصلها              OUTSIDE             \n",
      "القناة               OUTSIDE              القناة               OUTSIDE             \n",
      "الإنجليزية           OUTSIDE              الإنجليزية           OUTSIDE             \n",
      "عن                   OUTSIDE              عن                   OUTSIDE             \n",
      "القارة               OUTSIDE              القارة               OUTSIDE             \n",
      "الأوروبية            OUTSIDE              الأوروبية            OUTSIDE             \n",
      "جنوبا                OUTSIDE              جنوبا                OUTSIDE             \n",
      ".                    OUTSIDE              .                    OUTSIDE             \n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "#     index = 4054\n",
    "#     index = 5105\n",
    "#     index = 6948\n",
    "#     index = 9796\n",
    "#     index = 12370\n",
    "#     index = 14121\n",
    "#     index = 18021\n",
    "# index = 20745\n",
    "# index = 253\n",
    "# index = 6919\n",
    "# index = 12612\n",
    "# index = 12617\n",
    "# index = 20745\n",
    "curr_tokens, curr_predicted_labels, curr_confidence_scores = predictor.predict(' '.join(sentences[index]))\n",
    "\n",
    "# print(f\"{'Original Token':<20} {'True Label':<20} {'Word':<20} {'Prediction':<20} {'UpdatedLabels':<20}\")\n",
    "print(f\"{'Original Token':<20} {'True Label':<20} {'Word':<20} {'Prediction':<20}\")\n",
    "\n",
    "\n",
    "for i in range(0, len(sentences[index])):\n",
    "    print(f\"{sentences[index][i]:<20} {labels[index][i]:<20} {curr_tokens[i]:<20} {curr_predicted_labels[i]:<20}\")\n",
    "    # print(f\"{sentences[index][i]:<20} {labels[index][i]:<20} {curr_tokens[i]:<20} {curr_predicted_labels[i]:<20} {updated_labels[index][i]:<20}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd3027e1-4635-4bcd-a0ec-00de8c80d02d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21724 21724 21724\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences), len(predictions), len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9bb2e81d-b69b-426b-81be-840f1a9f2689",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Snippet to check if we have an error at some index\n",
    "for i in range(0, len(sentences)):\n",
    "    # print(len(sentences[i]), len(labels[i]), len(predictions[i]))\n",
    "    if len(labels[i]) != len(predictions[i]):\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02794884-2f04-40a7-b1c0-232095a4b6bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "updated_labels = []\n",
    "# len(sentences)\n",
    "\n",
    "for i in range(0, len(sentences)):\n",
    "    sentence = sentences[i]\n",
    "    sentence_labels = labels[i]\n",
    "    label_confidence = confidence_scores[i]\n",
    "    predictedLabels = predictions[i]\n",
    "    nonLabeledCount = 0\n",
    "    nonLabeledIndex = []\n",
    "    averageConfidence = 0\n",
    "    threshold = 0.80\n",
    "    tmpCurrLabels = []\n",
    "    \n",
    "    for j in range(0, len(sentence_labels)):\n",
    "        curr_label = sentence_labels[j]\n",
    "        \n",
    "        if curr_label == \"OUTSIDE\":\n",
    "            nonLabeledCount += 1\n",
    "            nonLabeledIndex.append(j)\n",
    "            \n",
    "    for k in range(0, len(label_confidence)):\n",
    "        if k in nonLabeledIndex:\n",
    "            averageConfidence += label_confidence[k]\n",
    "    \n",
    "    averageConfidence = averageConfidence / nonLabeledCount\n",
    "    \n",
    "    if nonLabeledCount > 0 and averageConfidence >= threshold:\n",
    "        for c in range(0, len(sentence_labels)):\n",
    "            if c not in nonLabeledIndex:\n",
    "                tmpCurrLabels.append(sentence_labels[c])\n",
    "\n",
    "            else:\n",
    "                # print(c)\n",
    "                tmpCurrLabels.append(predictedLabels[c])\n",
    "    else:\n",
    "        tmpCurrLabels = sentence_labels.copy()\n",
    "        \n",
    "    updated_labels.append(tmpCurrLabels)\n",
    "            \n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef1f0a90-02f5-4d7d-872f-f9c1d63f92f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('Labeled_SemiLabeledData.txt', 'w') as file:\n",
    "    for i in range(0, len(updated_labels)):\n",
    "        for j in range(0, len(updated_labels[i])):\n",
    "            file.write(f\"{updated_labels[i][j]} {sentences[i][j]}\\n\")\n",
    "        file.write('\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
