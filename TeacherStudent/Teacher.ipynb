{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "512e06e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mke37/.local/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "2024-06-12 22:53:40.808019: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-12 22:53:40.808145: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-12 22:53:40.809912: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-12 22:53:40.833728: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-12 22:53:46.592653: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/mke37/.local/lib/python3.10/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from preprocess import NERDataset\n",
    "from cleaning import DataReader\n",
    "import numpy as np\n",
    "from utils import compute_metrics, get_label_map, get_inv_label_map, read_labels\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, BertForTokenClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "from config import Config\n",
    "\n",
    "from transformers import AutoTokenizer, BertForTokenClassification # Import AutoTokenizer and BertForTokenClassification from the transformers library for NLP tasks.\n",
    "import torch # Import the PyTorch library for tensor computations and deep learning.\n",
    "import numpy as np # Import NumPy for numerical operations and array manipulations.\n",
    "import argparse # Import argparse for parsing command-line arguments.\n",
    "from typing import List # Import List from the typing module for type annotations.\n",
    "from config import Config # Import Config class from the config module, used for loading and accessing configuration settings.\n",
    "# Import utility functions: read_labels (to read label data), get_label_map and get_inv_label_map (for mapping labels to indices and vice versa).\n",
    "from utils import read_labels, get_label_map, get_inv_label_map\n",
    "import argparse # Re-import argparse (duplicate import, not necessary).\n",
    "import sys # Import sys for interacting with the Python interpreter (e.g., command-line arguments, system exit).\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "import re\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "981fe364-a328-428b-a3a4-4e20b247a353",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "labels = []\n",
    "\n",
    "curr_sentence = []\n",
    "curr_labels = []\n",
    "\n",
    "with open(\"LabeledArticles_G.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        if line != \"\\n\":\n",
    "            label = line.split()[0]\n",
    "            word = line.split()[1]\n",
    "            \n",
    "            curr_sentence.append(word)\n",
    "            curr_labels.append(label)\n",
    "        else:\n",
    "            sentences.append(curr_sentence)\n",
    "            labels.append(curr_labels)\n",
    "            curr_sentence = []\n",
    "            curr_labels = []\n",
    "            \n",
    "print(\"DONE!\")           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4d6ee6f-85ed-48bf-8dda-d342f0f854e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5545 5545\n",
      "['ولهذا', 'السبب', 'يمكن', 'وصف', 'مثبطات', 'إعادة', 'امتصاص', 'السيروتونين', 'الانتقائية', 'لعلاج', 'سرعة', 'القذف', '.']\n",
      "['OUTSIDE', 'OUTSIDE', 'OUTSIDE', 'OUTSIDE', 'OUTSIDE', 'OUTSIDE', 'OUTSIDE', 'OUTSIDE', 'OUTSIDE', 'OUTSIDE', 'OUTSIDE', 'OUTSIDE', 'OUTSIDE']\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences), len(labels))\n",
    "print(sentences[-1])\n",
    "print(labels[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b29445eb-47f8-4547-a7ee-959a7ff9460a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_diacritics(text):\n",
    "    return re.sub(re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]'),\"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "812d67ae-7f4f-43e5-a115-0e25656e13f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences[i])):\n",
    "        sentences[i][j] = remove_diacritics(sentences[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6eaee22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: 5545 , Sentences: 5545 , Tags: 5545\n",
      "Data: 856 , Sentences: 856 , Tags: 856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/sw/miniconda/envs/transformers-r1/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0126289976484cc3aa310524ccfb62e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4689902917881864\n",
      "Evaluating Epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad8bf936f46443d7952b8c1f2024e63b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mke37/.local/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: OUTSIDE seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 0.6059350295475235\n",
      "Eval Metrics: {'accuracy_score': 0.8731750479280342, 'precision': 0.592638036809816, 'recall': 0.4192708333333333, 'f1': 0.49110320284697506}\n",
      "Training Epoch: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f44563f711b451d8571dbc718804f50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.21957403593173289\n",
      "Evaluating Epoch: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71ea8c5d9bee45cca77884244e51f237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 0.6039681938235406\n",
      "Eval Metrics: {'accuracy_score': 0.8803519638204788, 'precision': 0.626733921815889, 'recall': 0.4314236111111111, 'f1': 0.5110539845758355}\n",
      "Training Epoch: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d55479887d45fd87089e9fe5002480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.15388356267289402\n",
      "Evaluating Epoch: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47cfd3fd922c462d8bc4a5bc73acdd8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 0.6255987374870865\n",
      "Eval Metrics: {'accuracy_score': 0.8830064395615199, 'precision': 0.5871866295264624, 'recall': 0.4574652777777778, 'f1': 0.5142717736033179}\n",
      "Training Epoch: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765f8f2595d443d082bb8962f08fbb5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.11581500417281812\n",
      "Evaluating Epoch: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a2b6c11a8f42b9be8e610fa99598b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 0.7595524376741162\n",
      "Eval Metrics: {'accuracy_score': 0.8839404217667011, 'precision': 0.6110134739308729, 'recall': 0.4526909722222222, 'f1': 0.5200698080279232}\n",
      "Training Epoch: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "114c2e7ba3eb4413bb1d6005a8dbb062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.09028154863876636\n",
      "Evaluating Epoch: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a0c86d12ad74b438729ffb1321dafc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 0.809215217138882\n",
      "Eval Metrics: {'accuracy_score': 0.8823182421471759, 'precision': 0.5975609756097561, 'recall': 0.4466145833333333, 'f1': 0.511177347242921}\n",
      "Training Epoch: 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9289a1f2c6044418be5504934168fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.07631284804472577\n",
      "Evaluating Epoch: 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a157d83b76a4b8a837bd92814906f51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 0.7961839575458456\n",
      "Eval Metrics: {'accuracy_score': 0.8846286191810451, 'precision': 0.5923248053392659, 'recall': 0.4622395833333333, 'f1': 0.5192588980984885}\n",
      "Training Epoch: 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a945ae71b9ef49488d8123c8255a04a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.06416562255955052\n",
      "Evaluating Epoch: 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df6a3fef273d4a6484266b25d0c2fe9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 0.8373791569912875\n",
      "Eval Metrics: {'accuracy_score': 0.8842845204738731, 'precision': 0.5769230769230769, 'recall': 0.4752604166666667, 'f1': 0.5211803902903379}\n",
      "Training Epoch: 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9d0fc6929614747a2b737cd2759d52a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.05651189106105899\n",
      "Evaluating Epoch: 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff243ef2e9324501a4ec84c2dbe9469f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 0.8437369910103304\n",
      "Eval Metrics: {'accuracy_score': 0.8840387356830359, 'precision': 0.5982192543127435, 'recall': 0.4665798611111111, 'f1': 0.5242623750304805}\n",
      "Training Epoch: 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f6ce87997b7402387f040d021a8220a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.051024890912734294\n",
      "Evaluating Epoch: 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f8a1db0d8a743a9a43f0aafab12d38f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 0.854430982084186\n",
      "Eval Metrics: {'accuracy_score': 0.8852676596372216, 'precision': 0.5929108485499462, 'recall': 0.4791666666666667, 'f1': 0.530004800768123}\n",
      "Training Epoch: 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c67cefd100d4e4eafa2f7efa4ba6cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.04653405823532803\n",
      "Evaluating Epoch: 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d54cbc2502a645db8522d9668b43c256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 0.9700526621330667\n",
      "Eval Metrics: {'accuracy_score': 0.8850218748463845, 'precision': 0.6017997750281214, 'recall': 0.4644097222222222, 'f1': 0.5242528172464477}\n",
      "Training Epoch: 11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b73c3760e62461da0aea363069c233e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.042946181708265106\n",
      "Evaluating Epoch: 11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a05e7750eb4629bc2c1fbd9ac7f3b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 0.9584150235686038\n",
      "Eval Metrics: {'accuracy_score': 0.8841862065575382, 'precision': 0.6114318706697459, 'recall': 0.4596354166666667, 'f1': 0.5247770069375619}\n",
      "Training Epoch: 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c5c8c45e949458682345a68414f5308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.03935922264699859\n",
      "Evaluating Epoch: 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2d8a198d33240faa363f27d003375c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 0.9524180121995784\n",
      "Eval Metrics: {'accuracy_score': 0.8839895787248685, 'precision': 0.5943600867678959, 'recall': 0.4756944444444444, 'f1': 0.5284474445515912}\n",
      "Training Epoch: 13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d10c36c8165641028d2776ba7ff3ea1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.03613666059168845\n",
      "Evaluating Epoch: 13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9079dff3abc462f94ce72c697efe06b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 0.9665944060241735\n",
      "Eval Metrics: {'accuracy_score': 0.8841862065575382, 'precision': 0.5970982142857143, 'recall': 0.4644097222222222, 'f1': 0.5224609375}\n",
      "Training Epoch: 14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ac823681e4840568c3d462e0980981b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.034737942031093524\n",
      "Evaluating Epoch: 14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7988d43c4e5645e8a0f9ed49bb52a8c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 1.0519982190043837\n",
      "Eval Metrics: {'accuracy_score': 0.8831539104360222, 'precision': 0.6087209302325581, 'recall': 0.4544270833333333, 'f1': 0.5203777335984094}\n",
      "Training Epoch: 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da19ef29649f4862babd026c89319470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.03224588186555255\n",
      "Evaluating Epoch: 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12712530eaf24034875e8c3c4fc12c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 1.048440913911219\n",
      "Eval Metrics: {'accuracy_score': 0.8837437939340314, 'precision': 0.5962732919254659, 'recall': 0.4583333333333333, 'f1': 0.5182822085889571}\n",
      "Training Epoch: 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "243037580ba546f3a0ad89a21ec904c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.03060057525229788\n",
      "Evaluating Epoch: 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "680b6b4c877d43ad9f863eaa195f0ce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 1.0977225838987916\n",
      "Eval Metrics: {'accuracy_score': 0.8839895787248685, 'precision': 0.6069819819819819, 'recall': 0.4678819444444444, 'f1': 0.5284313725490195}\n",
      "Training Epoch: 17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9775510dc1df49d39f31c47047e42d2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.029367484422722665\n",
      "Evaluating Epoch: 17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac12817b97ca44968987f8047c300466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 1.110070372345271\n",
      "Eval Metrics: {'accuracy_score': 0.8830555965196873, 'precision': 0.60250569476082, 'recall': 0.4592013888888889, 'f1': 0.5211822660098523}\n",
      "Training Epoch: 18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d254b98302474e0eae277b61f7ea26d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.027367006767922288\n",
      "Evaluating Epoch: 18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "589464f3305041b0a6d1187d4575df89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 1.117373406611107\n",
      "Eval Metrics: {'accuracy_score': 0.8830555965196873, 'precision': 0.5958751393534002, 'recall': 0.4639756944444444, 'f1': 0.5217179111761836}\n",
      "Training Epoch: 19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d542cc6e0aed4005b2a052706a4d5144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.02556927624575912\n",
      "Evaluating Epoch: 19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f72e047fda64f269e3d86e1cc1870db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 1.122582860842899\n",
      "Eval Metrics: {'accuracy_score': 0.8835471661013616, 'precision': 0.5973229224762967, 'recall': 0.46484375, 'f1': 0.5228215767634855}\n",
      "Training Epoch: 20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c846b031e0c461d9271ed59f364aeb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.024970477223309074\n",
      "Evaluating Epoch: 20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b1f2b657db0423fb3ab6d025d252496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 1.126682222717338\n",
      "Eval Metrics: {'accuracy_score': 0.8836454800176965, 'precision': 0.5992196209587514, 'recall': 0.4665798611111111, 'f1': 0.52464616886286}\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "class NERTrainer:\n",
    "    def __init__(self, test_dataset_path):\n",
    "        self.cfg = Config()\n",
    "        self.data_reader = DataReader(\"LabeledArticles_G.txt\")\n",
    "        self.data, _, _ = self.data_reader.read_data_bert()\n",
    "        self.label_list = read_labels('NewEntities.txt')\n",
    "\n",
    "        self.label_map = get_label_map(self.label_list)\n",
    "        self.inv_label_map = get_inv_label_map(self.label_list)\n",
    "\n",
    "        # Load the test dataset\n",
    "        self.test_data_reader = DataReader(test_dataset_path)\n",
    "        self.test_data, _, _ = self.test_data_reader.read_data_bert()\n",
    "\n",
    "        self.TOKENIZER = AutoTokenizer.from_pretrained(self.cfg.MODEL_NAME)\n",
    "\n",
    "        self.train_dataset = NERDataset(\n",
    "            texts=[x[0] for x in self.data],\n",
    "            tags=[x[1] for x in self.data],\n",
    "            label_list=self.label_list,\n",
    "            model_name=self.cfg.MODEL_NAME,\n",
    "            max_length=self.cfg.MAX_LEN\n",
    "        )\n",
    "\n",
    "        self.test_dataset = NERDataset(\n",
    "            texts=[x[0] for x in self.test_data],\n",
    "            tags=[x[1] for x in self.test_data],\n",
    "            label_list=self.label_list,\n",
    "            model_name=self.cfg.MODEL_NAME,\n",
    "            max_length=self.cfg.MAX_LEN\n",
    "        )\n",
    "\n",
    "        self.train_data_loader = DataLoader(dataset=self.train_dataset, batch_size=self.cfg.TRAIN_BATCH_SIZE, shuffle=True)\n",
    "        self.test_data_loader = DataLoader(dataset=self.test_dataset, batch_size=self.cfg.VALID_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        self.model = BertForTokenClassification.from_pretrained(self.cfg.MODEL_NAME,\n",
    "                                                                return_dict=True,\n",
    "                                                                num_labels=len(self.label_map),\n",
    "                                                                output_attentions=False,\n",
    "                                                                output_hidden_states=False).to(self.cfg.device)\n",
    "\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=5e-5, correct_bias=False)\n",
    "        total_steps = len(self.train_data_loader) * self.cfg.EPOCHS\n",
    "\n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "\n",
    "        self.best_eval_loss = float('inf')\n",
    "        self.best_model = None\n",
    "\n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        final_loss = 0\n",
    "\n",
    "        for data in tqdm(self.train_data_loader, total=len(self.train_data_loader)):\n",
    "            input_ids = data['input_ids'].to(self.cfg.device)\n",
    "            attention_mask = data['attention_mask'].to(self.cfg.device)\n",
    "            token_type_ids = data['token_type_ids'].to(self.cfg.device)\n",
    "            labels = data['labels'].to(self.cfg.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(input_ids=input_ids,\n",
    "                                 token_type_ids=token_type_ids,\n",
    "                                 attention_mask=attention_mask,\n",
    "                                 labels=labels)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            final_loss += loss.item()\n",
    "\n",
    "        loss = final_loss / len(self.train_data_loader)\n",
    "        print(f\"Train loss: {loss}\")\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def eval_epoch(self):\n",
    "        self.model.eval()\n",
    "        final_loss = 0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in tqdm(self.test_data_loader, total=len(self.test_data_loader)):\n",
    "                input_ids = data['input_ids'].to(self.cfg.device)\n",
    "                attention_mask = data['attention_mask'].to(self.cfg.device)\n",
    "                token_type_ids = data['token_type_ids'].to(self.cfg.device)\n",
    "                labels = data['labels'].to(self.cfg.device)\n",
    "\n",
    "                outputs = self.model(input_ids=input_ids,\n",
    "                                     token_type_ids=token_type_ids,\n",
    "                                     attention_mask=attention_mask,\n",
    "                                     labels=labels)\n",
    "\n",
    "                loss = outputs.loss\n",
    "                final_loss += loss.item()\n",
    "\n",
    "                logits = outputs.logits.detach().cpu().numpy()\n",
    "                labels = labels.to('cpu').numpy()\n",
    "\n",
    "                all_preds.extend(logits)\n",
    "                all_labels.extend(labels)\n",
    "\n",
    "        all_preds = np.array(all_preds)\n",
    "        all_labels = np.asarray(all_labels)\n",
    "\n",
    "        metrics = compute_metrics(all_preds, all_labels, self.inv_label_map, False)\n",
    "        final_loss = final_loss / len(self.test_data_loader)\n",
    "\n",
    "        print(f\"Eval loss: {final_loss}\")\n",
    "        print(f\"Eval Metrics: {metrics}\")\n",
    "\n",
    "        return final_loss, metrics\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.cfg.EPOCHS):\n",
    "            print(f\"Training Epoch: {epoch + 1}\")\n",
    "            self.train_epoch()\n",
    "\n",
    "            print(f\"Evaluating Epoch: {epoch + 1}\")\n",
    "            eval_loss, _ = self.eval_epoch()\n",
    "\n",
    "            if eval_loss < self.best_eval_loss:\n",
    "                self.best_eval_loss = eval_loss\n",
    "                self.best_model = self.model.state_dict()\n",
    "                torch.save(self.best_model, \"TeacherG_Model.pt\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_dataset_path = \"TestingData.txt\"  # Path to the custom test dataset\n",
    "    ner_trainer = NERTrainer(test_dataset_path)\n",
    "    ner_trainer.train()\n",
    "    print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e446e9f1-e1db-4115-85d5-3d5963b5682e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "labels = []\n",
    "\n",
    "curr_sentence = []\n",
    "curr_labels = []\n",
    "\n",
    "with open(\"SemiLabeledArticles_G.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        if line != \"\\n\":\n",
    "            label = line.split()[0]\n",
    "            word = line.split()[1]\n",
    "            \n",
    "            curr_sentence.append(word)\n",
    "            curr_labels.append(label)\n",
    "        else:\n",
    "            sentences.append(curr_sentence)\n",
    "            labels.append(curr_labels)\n",
    "            curr_sentence = []\n",
    "            curr_labels = []\n",
    "            \n",
    "print(\"DONE!\")           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e007cbce-be99-4b85-b0dd-2457b874b6a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21726 21726\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences), len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e73bfd15-f1fb-42ee-a417-34d9998e9349",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_diacritics(text):\n",
    "    return re.sub(re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]'),\"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d81b7084-a4c8-4bc8-8a6b-614491275aeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 %\n",
      "0.46 %\n",
      "0.92 %\n",
      "1.38 %\n",
      "1.84 %\n",
      "2.3 %\n",
      "2.76 %\n",
      "3.22 %\n",
      "3.68 %\n",
      "4.14 %\n",
      "4.6 %\n",
      "5.06 %\n",
      "5.52 %\n",
      "5.98 %\n",
      "6.44 %\n",
      "6.9 %\n",
      "7.36 %\n",
      "7.82 %\n",
      "8.29 %\n",
      "8.75 %\n",
      "9.21 %\n",
      "9.67 %\n",
      "10.13 %\n",
      "10.59 %\n",
      "11.05 %\n",
      "11.51 %\n",
      "11.97 %\n",
      "12.43 %\n",
      "12.89 %\n",
      "13.35 %\n",
      "13.81 %\n",
      "14.27 %\n",
      "14.73 %\n",
      "15.19 %\n",
      "15.65 %\n",
      "16.11 %\n",
      "16.57 %\n",
      "17.03 %\n",
      "17.49 %\n",
      "17.95 %\n",
      "18.41 %\n",
      "18.87 %\n",
      "19.33 %\n",
      "19.79 %\n",
      "20.25 %\n",
      "20.71 %\n",
      "21.17 %\n",
      "21.63 %\n",
      "22.09 %\n",
      "22.55 %\n",
      "23.01 %\n",
      "23.47 %\n",
      "23.93 %\n",
      "24.39 %\n",
      "24.86 %\n",
      "25.32 %\n",
      "25.78 %\n",
      "26.24 %\n",
      "26.7 %\n",
      "27.16 %\n",
      "27.62 %\n",
      "28.08 %\n",
      "28.54 %\n",
      "29.0 %\n",
      "29.46 %\n",
      "29.92 %\n",
      "30.38 %\n",
      "30.84 %\n",
      "31.3 %\n",
      "31.76 %\n",
      "32.22 %\n",
      "32.68 %\n",
      "33.14 %\n",
      "33.6 %\n",
      "34.06 %\n",
      "34.52 %\n",
      "34.98 %\n",
      "35.44 %\n",
      "35.9 %\n",
      "36.36 %\n",
      "36.82 %\n",
      "37.28 %\n",
      "37.74 %\n",
      "38.2 %\n",
      "38.66 %\n",
      "39.12 %\n",
      "39.58 %\n",
      "40.04 %\n",
      "40.5 %\n",
      "40.96 %\n",
      "41.43 %\n",
      "41.89 %\n",
      "42.35 %\n",
      "42.81 %\n",
      "43.27 %\n",
      "43.73 %\n",
      "44.19 %\n",
      "44.65 %\n",
      "45.11 %\n",
      "45.57 %\n",
      "46.03 %\n",
      "46.49 %\n",
      "46.95 %\n",
      "47.41 %\n",
      "47.87 %\n",
      "48.33 %\n",
      "48.79 %\n",
      "49.25 %\n",
      "49.71 %\n",
      "51.09 %\n",
      "51.55 %\n",
      "52.01 %\n",
      "52.47 %\n",
      "52.93 %\n",
      "53.39 %\n",
      "53.85 %\n",
      "54.31 %\n",
      "54.77 %\n",
      "55.23 %\n",
      "55.69 %\n",
      "56.15 %\n",
      "56.61 %\n",
      "57.07 %\n",
      "57.53 %\n",
      "58.0 %\n",
      "58.46 %\n",
      "58.92 %\n",
      "59.38 %\n",
      "59.84 %\n",
      "60.3 %\n",
      "60.76 %\n",
      "61.22 %\n",
      "61.68 %\n",
      "62.14 %\n",
      "62.6 %\n",
      "63.06 %\n",
      "63.52 %\n",
      "63.98 %\n",
      "64.44 %\n",
      "64.9 %\n",
      "65.36 %\n",
      "65.82 %\n",
      "66.28 %\n",
      "66.74 %\n",
      "67.2 %\n",
      "67.66 %\n",
      "68.12 %\n",
      "68.58 %\n",
      "69.04 %\n",
      "69.5 %\n",
      "69.96 %\n",
      "70.42 %\n",
      "70.88 %\n",
      "71.34 %\n",
      "71.8 %\n",
      "72.26 %\n",
      "72.72 %\n",
      "73.18 %\n",
      "73.64 %\n",
      "74.1 %\n",
      "74.57 %\n",
      "75.03 %\n",
      "75.49 %\n",
      "75.95 %\n",
      "76.41 %\n",
      "76.87 %\n",
      "77.33 %\n",
      "77.79 %\n",
      "78.25 %\n",
      "78.71 %\n",
      "79.17 %\n",
      "79.63 %\n",
      "80.09 %\n",
      "80.55 %\n",
      "81.01 %\n",
      "81.47 %\n",
      "81.93 %\n",
      "82.39 %\n",
      "82.85 %\n",
      "83.31 %\n",
      "83.77 %\n",
      "84.23 %\n",
      "84.69 %\n",
      "85.15 %\n",
      "85.61 %\n",
      "86.07 %\n",
      "86.53 %\n",
      "86.99 %\n",
      "87.45 %\n",
      "87.91 %\n",
      "88.37 %\n",
      "88.83 %\n",
      "89.29 %\n",
      "89.75 %\n",
      "90.21 %\n",
      "90.67 %\n",
      "91.14 %\n",
      "91.6 %\n",
      "92.06 %\n",
      "92.52 %\n",
      "92.98 %\n",
      "93.44 %\n",
      "93.9 %\n",
      "94.36 %\n",
      "94.82 %\n",
      "95.28 %\n",
      "95.74 %\n",
      "96.2 %\n",
      "96.66 %\n",
      "97.12 %\n",
      "97.58 %\n",
      "98.04 %\n",
      "98.5 %\n",
      "98.96 %\n",
      "99.42 %\n",
      "99.88 %\n",
      "100 %\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "class NERPredictor:\n",
    "    def __init__(self, model_path: str):\n",
    "        self.cfg = Config() # Initialize and load configuration settings from the Config class.\n",
    "        \n",
    "        # Read the label list from the specified file path.\n",
    "        self.label_list = read_labels('NewEntities.txt')\n",
    "        # Create mappings from labels to indices and vice versa.\n",
    "        self.label_map = get_label_map(self.label_list)\n",
    "        self.inv_label_map = get_inv_label_map(self.label_list)\n",
    "\n",
    "        # Load the pre-trained BERT model for token classification.\n",
    "        self.model = BertForTokenClassification.from_pretrained(\n",
    "            self.cfg.MODEL_NAME,\n",
    "            return_dict=True,\n",
    "            num_labels=len(self.label_map),\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False\n",
    "        )\n",
    "\n",
    "        # Load the saved model weights.\n",
    "        self.model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "        # Load the tokenizer associated with the pre-trained BERT model.\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.cfg.MODEL_NAME)\n",
    "\n",
    "    def predict(self, sentences: str) -> Tuple[List[str], List[float]]:\n",
    "\n",
    "        \n",
    "        foundDecimal = False\n",
    "        \n",
    "        tokenArray = sentences.split()\n",
    "        \n",
    "        for i in range(0, len(tokenArray)):\n",
    "            tokenArray[i] = remove_diacritics(tokenArray[i])\n",
    "            \n",
    "        sentences = ' '.join(tokenArray)\n",
    "        # print(sentences)\n",
    "        \n",
    "        # Tokenize the input sentence to get input IDs.\n",
    "        input_ids = self.tokenizer.encode(sentences, return_tensors='pt')\n",
    "        # print(len(input_ids[0]), input_ids)\n",
    "        with torch.no_grad(): # Disable gradient calculations for inference.\n",
    "            self.model.to('cpu') # Ensure the model is on CPU for inference.\n",
    "            # Get model predictions for the input IDs.\n",
    "            output = self.model(input_ids)\n",
    "\n",
    "        logits = output.logits.to('cpu')  # Get logits from the model's output\n",
    "        probabilities = F.softmax(logits, dim=-1)  # Apply softmax to get probabilities\n",
    "       \n",
    "        # Extract predicted labels and confidence scores\n",
    "        label_indices = torch.argmax(logits, dim=-1).numpy()\n",
    "        max_probabilities, predicted_labels = torch.max(probabilities, dim=-1)\n",
    "        confidence_scores = max_probabilities.numpy()\n",
    "\n",
    "    \n",
    "        # Convert model logits to label indices.\n",
    "        # label_indices = np.argmax(output.logits.to('cpu').numpy(), axis=2)\n",
    "        # Convert input IDs back to tokens.\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
    "\n",
    "        new_tokens, new_labels, new_confidence_scores = [], [], []\n",
    "        merges = []\n",
    "        \n",
    "        new_confidence_scores = confidence_scores[0].copy()\n",
    "        \n",
    "        label_indices_0 = label_indices[0]\n",
    "        for i in range(len(tokens)):\n",
    "            token = tokens[i]\n",
    "            label_idx = label_indices_0[i]\n",
    "            # Merge subword tokens that start with \"##\".\n",
    "            # print(token, label_idx, \"DEBUG\", i)\n",
    "            # print(token, \"DEBUG\", i)\n",
    "\n",
    "            if foundDecimal:\n",
    "                # print(\"__________foundDecimal__________\")\n",
    "                token = \"##\" + tokens[i]\n",
    "                foundDecimal = False\n",
    "            if token.startswith(\".\") and tokens[i-1][-1].isdigit() and i != len(tokens)-2:\n",
    "                # print(\"__________foundDecimal__________\")\n",
    "                foundDecimal = True\n",
    "                token = \"##\" + tokens[i]\n",
    "            # Disabled below as that means there's an error in the sentences itself\n",
    "            if token == \"%\" or token == \"٪\" and i != 1:\n",
    "                # print(\"________foundPercentage_________\")\n",
    "                token = \"##\" + tokens[i]\n",
    "                # print(token)\n",
    "            if (token == \"٬\" or token == \"٫\") and (any(char.isdigit() for char in prev_token) and any(char.isdigit() for char in tokens[i+1])):\n",
    "                # print(\"___________foundComma___________\")\n",
    "                token = \"##\" + tokens[i]\n",
    "                foundDecimal = True\n",
    "                \n",
    "            elif (token == \"٬\" or token == \"٫\") and not (any(char.isdigit() for char in prev_token) and any(char.isdigit() for char in tokens[i+1])):\n",
    "                # print(\"___________foundComma___________\")\n",
    "                continue\n",
    "                \n",
    "            if token == \"؟\" or token == \"?\":\n",
    "                # print(\"___________foundQuestionMark___________\")\n",
    "                token = \"##\" + tokens[i]                            \n",
    "            if token.startswith(\"##\"):\n",
    "                merges.append(i)\n",
    "                # print(\"_____________MERGE_______________\")\n",
    "                new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "\n",
    "            else:\n",
    "                if input_ids[0][i] == 2 or input_ids[0][i] == 3:\n",
    "                    continue\n",
    "                # Append the label for the token to new_labels.\n",
    "                new_labels.append(self.inv_label_map[label_idx])\n",
    "                # Append the token to new_tokens.\n",
    "                new_tokens.append(token)\n",
    "\n",
    "\n",
    "                \n",
    "            prev_token = token   \n",
    "            \n",
    "        new_confidence_scores = confidence_scores[0].copy()\n",
    "\n",
    "        # Iterate through confidence scores\n",
    "        i = 0\n",
    "        while i < len(new_confidence_scores):\n",
    "            # Check if current index is a merge point\n",
    "            if i in merges:\n",
    "                # Determine which element to keep and delete the other\n",
    "                if new_confidence_scores[i] >= new_confidence_scores[i - 1]:\n",
    "                    new_confidence_scores = np.delete(new_confidence_scores, i - 1)\n",
    "                    i -= 1  # Adjust index after deletion\n",
    "                else:\n",
    "                    new_confidence_scores = np.delete(new_confidence_scores, i)\n",
    "                    i -= 1  # Adjust index after deletion\n",
    "                # Adjust merge indices\n",
    "                merges = [merge - 1 for merge in merges if merge > i]\n",
    "            i += 1  # Move to the next element\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "        # Return the list of labels corresponding to each token in the input.\n",
    "        return new_tokens, new_labels, new_confidence_scores[1:-1]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    predictor = NERPredictor(model_path='TeacherG_Model.pt')\n",
    "\n",
    "    # index = 10\n",
    "    # curr_tokens, curr_predicted_labels, curr_confidence_scores = predictor.predict(' '.join(sentences[index]))\n",
    "    # # predictions[index] = curr_predicted_labels\n",
    "    # print(len(curr_predicted_labels), len(curr_confidence_scores), len(sentences[index]))\n",
    "    # print(curr_tokens, curr_predicted_labels, curr_confidence_scores)       \n",
    "    \n",
    "    \n",
    "    predictions = []\n",
    "    confidence_scores = []\n",
    "    copySentences = []\n",
    "    for i in range(0, len(sentences)):\n",
    "        curr_tokens, curr_predicted_labels, curr_confidence_scores = predictor.predict(' '.join(sentences[i]))\n",
    "        predictions.append(curr_predicted_labels)\n",
    "        copySentences.append(curr_tokens)\n",
    "        confidence_scores.append(curr_confidence_scores)\n",
    "        if i % 100 == 0:\n",
    "            print(round((i/len(sentences)) * 100, 2), \"%\")\n",
    "        \n",
    "    print(\"100 %\")\n",
    "    print(\"Done!\")\n",
    "\n",
    "    # index = 4\n",
    "    # index = 4054\n",
    "    # index = 5105\n",
    "    # index = 6948\n",
    "    # index = 9796\n",
    "    # index = 12370\n",
    "    # index = 14121\n",
    "    # index = 18021\n",
    "    # index = 20745\n",
    "    # index = 253\n",
    "    # index = 261\n",
    "    # index = 268\n",
    "    \n",
    "#     curr_tokens, curr_predicted_labels, curr_confidence_scores = predictor.predict(' '.join(sentences[index]))\n",
    "\n",
    "#     # Print titles\n",
    "#     print(f\"{'Original Token':<20} {'True Label':<20} {'Word':<20} {'Prediction':<20}\")\n",
    "\n",
    "#     # Print tokens and corresponding labels\n",
    "#     for i in range(0, len(sentences[index])):\n",
    "#         print(f\"{sentences[index][i]:<20} {labels[index][i]:<20} {curr_tokens[i]:<20} {curr_predicted_labels[i]:<20}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db3c0443-b58c-41d2-a501-a95ba7118184",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "# dill.load_session('June9STeachingCompleted.db')\n",
    "dill.dump_session('TeachingG_Completed.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73bee9ab-8e49-45ac-98c8-ee75c87c6fec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Token       True Label           Word                 Prediction          \n",
      "إنجلترا              B-Country            إنجلترا              B-Country           \n",
      "أو                   OUTSIDE              أو                   OUTSIDE             \n",
      "إنكلترا              B-Country            إنكلترا              B-Country           \n",
      "أو                   OUTSIDE              أو                   OUTSIDE             \n",
      "إنكلترة              B-Country            إنكلترة              B-Country           \n",
      "بالإنجليزية          B-Language           بالإنجليزية          B-Language          \n",
      "إنگلاند              OUTSIDE              [UNK]                OUTSIDE             \n",
      "هي                   OUTSIDE              هي                   OUTSIDE             \n",
      "أكبر                 OUTSIDE              أكبر                 OUTSIDE             \n",
      "دولة                 OUTSIDE              دولة                 OUTSIDE             \n",
      "في                   OUTSIDE              في                   OUTSIDE             \n",
      "المملكة              B-Country            المملكة              B-Country           \n",
      "المتحدة              I-Country            المتحدة              I-Country           \n",
      "وتشترك               OUTSIDE              وتشترك               OUTSIDE             \n",
      "في                   OUTSIDE              في                   OUTSIDE             \n",
      "الحدود               OUTSIDE              الحدود               OUTSIDE             \n",
      "البرية               OUTSIDE              البرية               OUTSIDE             \n",
      "مع                   OUTSIDE              مع                   OUTSIDE             \n",
      "اسكتلندا             B-Country            اسكتلندا             B-Country           \n",
      "في                   OUTSIDE              في                   OUTSIDE             \n",
      "الشمال               OUTSIDE              الشمال               OUTSIDE             \n",
      "وويلز                OUTSIDE              وويلز                B-Country           \n",
      "في                   OUTSIDE              في                   OUTSIDE             \n",
      "الغرب                OUTSIDE              الغرب                OUTSIDE             \n",
      "والبحر               B-Body_Of_Water      والبحر               B-Body_Of_Water     \n",
      "الأيرلندي            I-Body_Of_Water      الأيرلندي            I-Body_Of_Water     \n",
      "في                   OUTSIDE              في                   OUTSIDE             \n",
      "الشمال               OUTSIDE              الشمال               OUTSIDE             \n",
      "الغربي               OUTSIDE              الغربي               OUTSIDE             \n",
      "وبحر                 B-Body_Of_Water      وبحر                 B-Body_Of_Water     \n",
      "الكلت                I-Body_Of_Water      الكلت                I-Body_Of_Water     \n",
      "في                   OUTSIDE              في                   OUTSIDE             \n",
      "الجنوب               OUTSIDE              الجنوب               OUTSIDE             \n",
      "الغربي               OUTSIDE              الغربي               OUTSIDE             \n",
      "وبحر                 B-Body_Of_Water      وبحر                 B-Body_Of_Water     \n",
      "الشمال               I-Body_Of_Water      الشمال               I-Body_Of_Water     \n",
      "في                   OUTSIDE              في                   OUTSIDE             \n",
      "الشرق                OUTSIDE              الشرق                OUTSIDE             \n",
      "وتفصلها              OUTSIDE              وتفصلها              OUTSIDE             \n",
      "القناة               OUTSIDE              القناة               OUTSIDE             \n",
      "الإنجليزية           B-Language           الإنجليزية           B-Language          \n",
      "عن                   OUTSIDE              عن                   OUTSIDE             \n",
      "القارة               OUTSIDE              القارة               OUTSIDE             \n",
      "الأوروبية            OUTSIDE              الأوروبية            OUTSIDE             \n",
      "جنوبا                OUTSIDE              جنوبا                OUTSIDE             \n",
      ".                    OUTSIDE              .                    OUTSIDE             \n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "#     index = 4054\n",
    "#     index = 5105\n",
    "#     index = 6948\n",
    "#     index = 9796\n",
    "#     index = 12370\n",
    "#     index = 14121\n",
    "#     index = 18021\n",
    "# index = 20745\n",
    "# index = 253\n",
    "# index = 6919\n",
    "# index = 12612\n",
    "# index = 12617\n",
    "# index = 20745\n",
    "curr_tokens, curr_predicted_labels, curr_confidence_scores = predictor.predict(' '.join(sentences[index]))\n",
    "\n",
    "# print(f\"{'Original Token':<20} {'True Label':<20} {'Word':<20} {'Prediction':<20} {'UpdatedLabels':<20}\")\n",
    "print(f\"{'Original Token':<20} {'True Label':<20} {'Word':<20} {'Prediction':<20}\")\n",
    "\n",
    "\n",
    "for i in range(0, len(sentences[index])):\n",
    "    print(f\"{sentences[index][i]:<20} {labels[index][i]:<20} {curr_tokens[i]:<20} {curr_predicted_labels[i]:<20}\")\n",
    "    # print(f\"{sentences[index][i]:<20} {labels[index][i]:<20} {curr_tokens[i]:<20} {curr_predicted_labels[i]:<20} {updated_labels[index][i]:<20}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd3027e1-4635-4bcd-a0ec-00de8c80d02d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21726 21726 21726\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences), len(predictions), len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bb2e81d-b69b-426b-81be-840f1a9f2689",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Snippet to check if we have an error at some index\n",
    "for i in range(0, len(sentences)):\n",
    "    # print(len(sentences[i]), len(labels[i]), len(predictions[i]))\n",
    "    if len(labels[i]) != len(predictions[i]):\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02794884-2f04-40a7-b1c0-232095a4b6bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "updated_labels = []\n",
    "# len(sentences)\n",
    "\n",
    "for i in range(0, len(sentences)):\n",
    "    sentence = sentences[i]\n",
    "    sentence_labels = labels[i]\n",
    "    label_confidence = confidence_scores[i]\n",
    "    predictedLabels = predictions[i]\n",
    "    nonLabeledCount = 0\n",
    "    nonLabeledIndex = []\n",
    "    averageConfidence = 0\n",
    "    threshold = 0.80\n",
    "    tmpCurrLabels = []\n",
    "    \n",
    "    for j in range(0, len(sentence_labels)):\n",
    "        curr_label = sentence_labels[j]\n",
    "        \n",
    "        if curr_label == \"OUTSIDE\":\n",
    "            nonLabeledCount += 1\n",
    "            nonLabeledIndex.append(j)\n",
    "            \n",
    "    for k in range(0, len(label_confidence)):\n",
    "        if k in nonLabeledIndex:\n",
    "            averageConfidence += label_confidence[k]\n",
    "    \n",
    "    averageConfidence = averageConfidence / nonLabeledCount\n",
    "    \n",
    "    if nonLabeledCount > 0 and averageConfidence >= threshold:\n",
    "        for c in range(0, len(sentence_labels)):\n",
    "            if c not in nonLabeledIndex:\n",
    "                tmpCurrLabels.append(sentence_labels[c])\n",
    "\n",
    "            else:\n",
    "                # print(c)\n",
    "                tmpCurrLabels.append(predictedLabels[c])\n",
    "    else:\n",
    "        tmpCurrLabels = sentence_labels.copy()\n",
    "        \n",
    "    updated_labels.append(tmpCurrLabels)\n",
    "            \n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef1f0a90-02f5-4d7d-872f-f9c1d63f92f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('Labeled_SemiLabeledData_G.txt', 'w') as file:\n",
    "    for i in range(0, len(updated_labels)):\n",
    "        for j in range(0, len(updated_labels[i])):\n",
    "            file.write(f\"{updated_labels[i][j]} {sentences[i][j]}\\n\")\n",
    "        file.write('\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
